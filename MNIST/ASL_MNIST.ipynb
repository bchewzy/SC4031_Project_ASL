{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5196a066-44aa-40be-9220-757a996637b4",
   "metadata": {},
   "source": [
    "## Import Base Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b105fd-98da-4894-abeb-d8adbe53c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20abbe-1e1a-498e-8e25-bf6f811a5e55",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce35c29e-15e2-49a9-a271-3fa372bad6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/sign_language_mnist/sign_mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"./input/sign_language_mnist/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a92b24f5-0388-4e27-8ad3-7e6b6fe4e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27455 entries, 0 to 27454\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 164.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99eb722c-d6f8-48f3-81dd-f37995a7895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 43.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>148</td>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>166</td>\n",
       "      <td>242</td>\n",
       "      <td>227</td>\n",
       "      <td>230</td>\n",
       "      <td>227</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>205</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>248</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>253</td>\n",
       "      <td>236</td>\n",
       "      <td>230</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      6     149     149     150     150     150     151     151     150   \n",
       "1      5     126     128     131     132     133     134     135     135   \n",
       "2     10      85      88      92      96     105     123     135     143   \n",
       "3      0     203     205     207     206     207     209     210     209   \n",
       "4      3     188     191     193     195     199     201     202     203   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     151  ...       138       148       127        89        82        96   \n",
       "1     136  ...        47       104       194       183       186       184   \n",
       "2     147  ...        68       166       242       227       230       227   \n",
       "3     210  ...       154       248       247       248       253       236   \n",
       "4     203  ...        26        40        64        48        29        46   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       106       112       120       107  \n",
       "1       184       184       182       180  \n",
       "2       226       225       224       222  \n",
       "3       230       240       253       255  \n",
       "4        49        46        46        53  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.info()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979d4a4-8a63-4b86-b002-ff3e231d1e2a",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a559c9-e01d-4740-a474-35c0c62433cc",
   "metadata": {},
   "source": [
    "### Split Labels and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "03f0960c-1cb3-4428-b267-abb610e134ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df[\"label\"]\n",
    "train_set = train_df.drop([\"label\"], axis=1)\n",
    "\n",
    "test_label = test_df[\"label\"]\n",
    "test_set = test_df.drop([\"label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b75a5c97-1057-45b0-b997-13fb66523668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fc34bd7c-c0b4-4d8e-b196-d70f71d15874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(train_label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51dd0b-c7ea-43d4-81f8-0aaf1d4f4c2b",
   "metadata": {},
   "source": [
    "### Reshape data from 1-D to 3-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17c0d14f-12f6-4232-8ca6-08783706d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 28, 28, 1)\n",
      "(7172, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(train_set.values, (-1, 28, 28, 1))\n",
    "X_test = np.reshape(test_set.values, (-1, 28, 28, 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d8de890-34fc-4947-be69-03ad588e88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset (divide ea. pixel by 255 so that pixel range = [0, 1]\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2002a406-afdd-43a6-9c66-d351bc17fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAFuCAYAAACle+D3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLD0lEQVR4nO2df3BdZZnHv01p0pam6S+SEElogdIiXX6VpoQCIlvtgqKVusKuoygigybuYGd0trOKI6ObGWd2ZXSK+oe26iyWwRFcWWVXU1pQW2ojIP0VKRTS0iZtgTRtaZuSnP0D7+W+3/Mk77knN7nvbb+fmQw8957znve85znn7T3f532eMVEURRBCCCFEcJQVuwNCCCGEsNEkLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIFE3SQgghRKBokhZCCCECRZO0EEIIESiapIUQQohA0SQthBBCBMqITdIrV67EzJkzMX78eCxcuBCbNm0aqUMJYSIfFCEgPxTDYcxI5O5+6KGH8MlPfhLf//73sXDhQtx///14+OGH0dHRgerq6iH3HRgYwN69e1FZWYkxY8YUumuiyERRhMOHD6Ourg5lZSP3Imc4PgjID09lRssHAfmhGJzEfhiNAI2NjVFzc3PW7u/vj+rq6qLW1lbvvrt3744A6O8U/9u9e/dIuF6W4fhgFMkPT4e/kfbBKJIf6m/4fngGCkxfXx/a29uxYsWK7GdlZWVYvHgxNmzYENv+xIkTOHHiRNaO/vbD/rbbbkN5eTkAYMqUKbH9KisrHfvMM8907IqKCsfOtJXLuHHj8rIB4Iwz3CEbO3bskDZvb21z/Phxx544caJ3H7aZkf6FkGFgYMC7TZTzsubo0aP4yEc+Ert+hSRfHwQG98Pvfve7mDBhAgB7zKM8X0RZv4a4Dd+1BeLjzte7v79/SNuC+2GdG3/ma9fyD26Dt7H2eeuttxybz7evr8+x169fH2sj88u1r68PDz300Ij6YOY4hfLDNWvWZJ8LSe5t331pXVv2TT5OkmuZ7/2Qdh+f3yXx9zT9+MUvfuHYbW1tjs1z1eTJk2NtZOait956C+3t7V4/LPgkffDgQfT396Ompsb5vKamBjt27Iht39raiq9//euxz8vLy7MnwxMuAIwfPz4vO8kkzdukmaT5+ySTNN8M/A8Oa5+RmKSTTCBMvpP0UMcqFPn6IDC4H06YMCH7cDzVJ2lus5Qmabate5fv75F+fVxIP5w4cWL2uaBJOu4PzEhN0uxDPEZJntM8J/j8sOCTdL6sWLECy5cvz9q9vb2or69HRUVFdkDSTHS+763P2LYGj7fxTcrWcfkfEP/7v//r2PPnz4/t09DQEPsslySTZZqHkm+ffB8Yoepqg/nhuHHjsg/8NH0fqTcavomc+2ptzz7DDzbrocX78HF4nzT/KLEesHxf+Sb28847L9bGnDlzAADHjh3DT3/609j3ITCYH5aVlWV9KYkf+vwuzT/I0/zjIMlzia8/vxVhG7B/dOXi8xcL3z98AWDmzJmOPW3aNMfmZ/vJkydjbWTenCb9h0TBJ+kZM2Zg7Nix6O7udj7v7u5GbW1tbPuKigrzl7IQacnXBwH5oSg88kNRCAr+z/zy8nLMnz/feVc/MDCAtrY2NDU1FfpwQsSQD4oQkB+KQjAir7uXL1+O22+/HVdeeSUaGxtx//334+jRo/j0pz89EocTIoZ8UISA/FAMlxGZpG+99VYcOHAA9957L7q6unDZZZfh8ccfjwVQCDFSyAdFCMgPxXAZscCxlpYWtLS0pN6/rKwsG1BgBY75oqyTBIH5ouyswAHeJk0wEfd1//79jn3o0KHYPr6ghkIEKBUiatTX7mgtDQOG74PA2z5i+V9SfIFV1jZpSBMh7tvH6isHu/giwpNEBDNJgtw4upcDdqxlLZl90kT+DodC+GFu4FjS7XNJE0GdBJ8P+QJtgbcD+XJ57rnnHHvz5s2xfa688krH5mvKAVvXXHNNrA0rqCsXa7zPP/98x546dapj83JatnM/S/osVe5uIYQQIlA0SQshhBCBoklaCCGECJSiJzMZjLFjx2b1DUuj8mV2SaIvFyKZiU8nsjQY1il6enoc2zrfJH0bDZJkIWJGSg8bDXIz3yU515HQ3IcbBwAULuOYz+/yzUhmHdfKJsX3ESez4H2sMTt48CAAOGk3S4UxY8Zkxz6JZm/tn0uhYiM4sxuv8e7t7XXsXbt2xdp45plnHPvFF1907Jdeeim2D8fxcJbGN99807Gt5DaDrVUfiqqqKsfmWIiuri7HPnz4cKyNzBhJkxZCCCFKHE3SQgghRKBokhZCCCECpSQ06STrlX0FNqyqOL4KVkm0cF8/rPWanZ2djn3kyBHHZt0jyXEKQSEKSFg6S6hFNZKQqwVaPpQkKf9Q2yfZpxBVsZJQCN0yyXptLpjA61WtOA4ubPD88887NmudvD0ArFu3zjxeKZDrhxZJ1rgnOUYurPNbZRdZk33ttdcc+9e//rVj83UDgAMHDjg268lsA3F/57ge7vsPfvCDWBv/+q//6tisp1v36qRJkxx79uzZjv300087tlUcJN9ENvolLYQQQgSKJmkhhBAiUDRJCyGEEIFSEpp0mnXShVhXbOl6vuOyFmTpOLx2jtuYMmWKt2+F0HnT5Or2adC+Nkczd3chyM0hb415vrnck7SRZox8a5yTrK1lzd1a45xkTbPve16nPGPGDMdmnRMAtmzZ4ti/+c1vHHv+/PmOfdZZZ8XayNxXlk4YOsPN3Z0G1mgnTJgQ2yaz9jzDU0895dh/+MMfHPuVV16JteHLjX/06NHYZ+zf7Lt8/qxZA3H/ThL7wfcq5xD/2c9+NuQxgHfWjieNGyitJ6YQQghxGqFJWgghhAgUTdJCCCFEoGiSFkIIIQIl2MCxcePGZYMB0hScSBJYxdukSYDC/eDgCitxAicz4UAZK0DDV9gjSQBXmgClJMEUPnL7VmqJTc4444whA1s4+GOkEsL4YF9lv7P6xeeVJKCK+8aBMdOnT3fsiRMnxto4duyYY+/du9ext23bFttnx44djj1v3jzHnjNnjmNb53LFFVeYxy8FfMlM8r23rbb4WnJSJes+eOyxxxz7j3/8o2NzYJlV3IQ/Y5/h4hlA/Px4m+rqase+6aabYm1YiaZySTJGHKD40Y9+1LFfeOGFWBuZAiIDAwNmUByjX9JCCCFEoGiSFkIIIQJFk7QQQggRKMFq0rmkSWaSplgG75NGX+TkJVz0HAA6Ojocu6GhwbGtZA7cF18CgGJhLd4vZfJNIlEIzT1NHICvwIwVG+ErbGHFRrDWy8UPWF/kggtAXF/et2+fY1ta4Qc+8AHH5iIFx48fd2zLD63zKRVyNWnLHwtRYIXjK/g6vPzyy7F9uLAJa9B8P1jFg9gP2XetuIbPfOYzjs3PUE4IZcUX8XGTJBfxxS1xAY4PfehDsTYy99Dx48fxzW9+03tM/ZIWQgghAkWTtBBCCBEomqSFEEKIQAlT2MTb7/4zGlkaTTrJukHfWuskBTZ4G9YoLD2F15JOnTrVsa1k8LyNr18WfH5pCsOz1pdE+8o9biHWXY8mueuk04xXIcZ8sH4NdZwkehvrg+Xl5Y5t+WFXV5djc3wFa9SWDsy+fPHFFzu2pUlzoQ6fnm6R8b1Sj5tIsubd9yyzxoCfVVxgg9ezA/Hr4vPvJM8LXvP+sY99LLbN3LlzHdtX+MXql+9ZZI0R78MxF9u3b3fsRYsWxdrIjAHfK4OhX9JCCCFEoGiSFkIIIQJFk7QQQggRKJqkhRBCiEAJNnAst8BGmqAvXyCZ9ZmvTWubNElU3vOe9zg2Jy9Zv359bJ/6+nrHXrhwoWMnCYbx9c0q7MABF9xGkkCo3DELNQnLYOSbzIRJEziWJCEKX28uGuErQAAAhw8fduxDhw45NgcOAcD+/fsdmwsEzJ8/37E56YjVN06QwkE/Fr6iJEMFBQ3nehYLXyBtvlg+xglAXn/9dcfmxCVAPJDQVzDCOi772Sc+8QnHvuiii2L7cNAVX/9CBGwm8RNut66uzrF5TAHgmWeeARBPwDNoPxJtJYQQQohRR5O0EEIIESh5T9JPPvkkbr75ZtTV1WHMmDF49NFHne+jKMK9996Ls88+GxMmTMDixYvNmppCpEU+KEJAfihGg7wFwqNHj+LSSy/FHXfcgVtuuSX2/be+9S185zvfwY9//GPMmjULX/3qV7FkyRJs27bNLBwxGLkJ5S0dw6cFs55QqGQmPk2a4QQRQFxz+6//+i/HtnS89vZ2x84UsM/AmiRrRQAwb948x/bp+IBfy2EtyNKck2iM+TBaPgi4fph0+1x4/NLoXHxtAeCNN95wbE7sz37X2dkZa4P15XPPPdexrWQmrKO9733vc2wuMGP1nfX0JBorjwmPM7dp+e1Qz5M0hOSHvnPi8bAKTrAPvfTSS45tFWnhfThRDfuLpVlzXzh+woq38T27+XyTxNv4dG0L1pwXLFjg2E888URsnxdffBGAPZ4WeU/SN954I2688UbzuyiKcP/99+MrX/kKPvzhDwMAfvKTn6CmpgaPPvoobrvttnwPJ0QM+aAIAfmhGA0Kqknv2rULXV1dWLx4cfazqqoqLFy4EBs2bDD3OXHiBHp7e50/IdKSxgcB+aEoLPJDUSgKOklnXrHy69qamhrz9SsAtLa2oqqqKvvHS42EyIc0PgjID0VhkR+KQlH0RasrVqzA8uXLs3Zvby/q6+udwgaWjufTC3wFOJJsY+3Da/rOPvtsx2Zdg9caAnFdhterWuvn+DgcgMLJ8bnwAQBceumlsc/yxbce1ZfIPtQCG4P5oU8L9GnMPu0UAA4ePOjYvB6V1xEDcS2QfYhtK5k/TyC8j3UteY0/a9++9dpAujWsHNfA7XIbQ/lZqD4IDO6HY8eOHbLf+WrSXFwFiF9L/hWf5Fc9P4eSaO9cUKW6utqxk+SA4PNPoieneZbxPlxA5sILL3TsV199NdbG+eefD+DttyZJKOgv6draWgBAd3e383l3d3f2O6aiogKTJ092/oRISxofBOSHorDID0WhKOgkPWvWLNTW1qKtrS37WW9vL55++mk0NTUV8lBCmMgHRQjID0WhyPt195EjR7Bz586svWvXLjz77LOYNm0aGhoacM899+Ab3/gGZs+enV12UFdXh6VLlxay3+I0Rj4oQkB+KEaDvCfpzZs3473vfW/Wzugnt99+O1avXo0vf/nLOHr0KO666y709PTgmmuuweOPP573usBcLC0myTpoXxs+DdrKd8yfsebMejMXBQeAP//5z47NmsyBAwdi+1xwwQWOPVTwCRBfVw0Af//3f+/YrP1Y5KvbWPpRrj5UiDWqo+mDZWVl2XMeau1tBh6fJGuCWdfi62JpV7zGkv1/+vTpjs3rVy1YT7Zyd2/dutWxWevmtfjWmHPfk+iJvP4+zVrrNNsORTGehUCye8iX84H9A4hfS15Hn2TdPD8vOJbAOvfZs2c7Nt8PVkyG7zmUZM1zvm0A+cdCXHbZZbE2MuNs5RCwyHuSvv7664cM9BgzZgzuu+8+3Hffffk2LUQi5IMiBOSHYjRQ7m4hhBAiUDRJCyGEEIGiSVoIIYQIlKInM0lCkoQIHFjCAn6SYhnTpk1zbE4YAcQDHzioh4/LwRfWZxx8ZgVKcAAaJxbgxBRW8gqu0nPXXXc5diGCK3xBOUkKTIREbhKJNIXjeR/LHzhQkBNCWAGMvqATDhTjNgHbz3Kxkurw9X7qqaccm31o0aJFsTY46CtJIBTvw/c7BygN5cshJzMZjHwLbLDfcaISDs4CgH379jk2J7c5cuRIbB/2kaRFI3LhZ2qS65Nv8hIrCMyH9azyFXLhflgFljJFOaxiI2Y/Em0lhBBCiFFHk7QQQggRKJqkhRBCiEAJVpPO1QKTJDfwFQG34CTzmcTnGaxEJNu3b3ds1hNZx7GKq8+cOdOxucCCpSdz0YU33njDsa+44grH5gQpAPDXv/7VsVlPsrRPn5bD425tX+hkJqNJWVlZXjo6nz/reFZikj179gy5jVUMgY/D+jLnjLb8gc+Lc0pb2iAfh/th9ZUphCbsSyIxVHGEUvNBxuo/X0vWhvn6W23wc4fjHlj3t47riwWykh1lNNrB9rH66tPgfclcLApRyIPnJeu+y9ybSZOZ6Je0EEIIESiapIUQQohA0SQthBBCBEqwmrSPfLUBS9fgAgI+XQ8A/vKXvzj2xo0bHXvbtm2OzTo3AFx77bWOzZqMdVzWKX16srVekfVxXiuZJnE/a0E+7afU1knnrk9NMj6s6/GYW+smzz77bMdmbcy6ljzuvA6a12B2dnYO0uN34DgHi5deesmx+XpefPHFjm3pmEyScfXFPiTRINOslQ2F3Bidwb7PhX3orLPOcmxrzTPHxnB8jRUrw2Pqu07W2nt+DvGzOwlp4gx8c4gV18BzhK/ghlWkJt8CG6X1xBRCCCFOIzRJCyGEEIGiSVoIIYQIFE3SQgghRKAEGziWG7BjBUz4ApA4CCBJAgUO0OEkEwDQ0dHh2Dt27HBsDix75ZVXYm28733vG/K4VnAFL4rnQDFfchcgnkSFx8QK2EiSjCAXKzgnt41SDhyz+s6BJBwMwsE3HFgIxANy+FpbhTA4EIiLw/C1tYJ+enp6huwrfw8AGzZscOw777zTsTnA0fIHDnLzFamx8BU6sPw0026p+SDgT6rDPsLBqBzAxEmYAODQoUOOzf5gPZc4MNBnW9e2ra3Nsfn5OHny5Ng+vudQkiAwDp7jwEnL/3kbHhMOAuVnLgBccMEFAFRgQwghhCh5NEkLIYQQgaJJWgghhAiUYDXpXA3G0mJ8mnMSXYv1MtYtuBg5AEyfPt2xp06dOmS/6uvrY22wjsFJBFgbAuJJMN71rnc5tk8LBYB58+Y5trXQnsk3SYC1fe64FqK4wmji0wJ5nFkrS5KYpLe317HZPyx/ePXVVx373e9+t2PPmDHDsa0kKuy77P/sl0Bcx1u5cqVj33zzzY49Z86cWBusnzc0NDi2pcEzaQrqZLT+UtSkc7HOlbVfHmPW/ZPoreyXVnwBH5djHzjOxSo4xMdds2aNYy9fvjy2Dx+H+8ZtWrFBvuQ9lh/ycbu6uhyb9XWr75l4ImssLErbW4UQQohTGE3SQgghRKBokhZCCCECJVhNOnd9ahp8RcCBuI7BtqUZcDJ4Xq/K+pqle/zpT39ybNaHLB373HPPdWzWy3nNnaV91tXVOTbrw5bm5NP6k6xPzW231DTpM844I6urWToejzuvX2ct0Frz6cPSkzkGgddf83FZfwbivspa+M6dO2P7cIENjtFgf7f84Q9/+INjs35uxUqw9ukr7DBp0qRYG5n4gaTrU0Mit8CGdW/zc4n9kH339ddfj7XBGjT7kAX3xVeAwoL9m/2DY2kA4JxzznHs/fv3O7Z1fgyv1+dzsZ7d+/btc2zu+9y5cx3bet5lfDdpwRf9khZCCCECRZO0EEIIESiapIUQQohACVqTzugZaYrCsxZgtcFr+Kz8rgxrPbW1tY7N2rGl6/H6U17jyjo3ENcyX3vtNcdm/cRqg/PK8pgk0Y98OoqlweQep9Q06bfeeiurh7JmB8TjFliT43XFrOECcW2M9eYkGiSv12bbWvO8a9euIdvkPPRA/B7hOAceI6sNXvfN96F1r7Jvss1++bvf/S7WxqJFi8zjlQIDAwPZc7RyaM+aNcux+Trx+l5Ll/fl6rb8kH2V4dgZK/eEb837j3/849g+F154oWNzjmz2S8unWMfmugzWOL/nPe9x7EsuucSxfX7p65OFfkkLIYQQgaJJWgghhAiUvCbp1tZWLFiwAJWVlaiursbSpUvNVwTNzc2YPn06Jk2ahGXLlqG7u7ugnRanN/JDUWzkg2K0yGuSXr9+PZqbm7Fx40b89re/xcmTJ/H+97/f0Te++MUv4le/+hUefvhhrF+/Hnv37sUtt9xS8I6L0xf5oSg28kExWuQVQfH444879urVq1FdXY329nZcd911OHToEH74wx/iwQcfxA033AAAWLVqFS666CJs3LgRV111VeJj5S7etwKaOAApTdJ8DoTgQAFL9OfF67wN98tKqsDBZrNnz3ZsDk4D/MlKeJ/Pfe5zsTZ4myRBDr6kEb7teZ9CFDcYTT/ct29fdtysxBycZIPhwDIrGIWDb/g4VqCTz//ZT63EFBw4xv2wCgzU1NQ4NgfGvfzyy942Pvaxjzk2J1qxxpSDmrjAyJNPPunYnNwFeCdAk4Oo0jCaPpjpc+YaWwlxeAz5evM5W2PA9y77qnUt+TMOjORENRycCMR9la+/9czgADS+Hzg4cffu3bE2XnzxxdhnuXz2s5+NfcYJgPg5nOR56XtmMMN6YmYGIlNxpb29HSdPnsTixYuz28ydOxcNDQ3YsGGD2caJEyfQ29vr/AmRD/JDUWwK4YOA/FDEST1JDwwM4J577sGiRYuyadu6urpQXl6OKVOmONvW1NTESnplaG1tRVVVVfbPSokpxGDID0WxKZQPAvJDESf1JN3c3IwtW7bEan/my4oVK3Do0KHsn/VaQojBkB+KYlMoHwTkhyJOqlX9LS0teOyxx/Dkk086ic5ra2vR19eHnp4e51+Q3d3dMR02Q0VFhZlQv7+/P6vNJUluwNuwbSXRsBbn+2C9kHUb1g8t/Yg1GNZ1LF2XtT9+DfaJT3zCsS+44IJYG0ya5CU8rpw0wWozt43hFE1hRsMPjx8/PuQ4sb5UWVnp2JyohHVgIK6fsv5qRQSzP/MvLo4/2LNnT6wN1i35Wlva2RVXXOHYS5cudeyqqirH5qQSQHyMOOGJFcexd+9ex3766acdm/29sbEx1kYmoYsvAUc+FNIHgcH9sKysLOuHrIsC/uQlbFtjwNskeT7y847fHLD/sw3E9eU0hT06OzsdmzVp9ksAuOmmmxybk0pZ+/Bxedx9RZvSkNcv6SiK0NLSgkceeQRr166NZbmZP38+xo0bh7a2tuxnHR0d6OzsRFNT07A7KwQgPxTFRz4oRou8fkk3NzfjwQcfxC9/+UtUVlZmtZWqqipMmDABVVVV+MxnPoPly5dj2rRpmDx5Mr7whS+gqakp72hGIQZDfiiKjXxQjBZ5TdLf+973AADXX3+98/mqVavwqU99CgDw7W9/G2VlZVi2bBlOnDiBJUuW4IEHHihIZ4UA5Iei+MgHxWiR1ySdpADF+PHjsXLlSqxcuTJ1pwB3nbSlY/o0Z19CfgvWaZKsaeV2eZ2gNWbcV9ZgrOT3XFCD10UuXLjQsXl9LhDXR7hvlm7Pn3EbrNH71gUWYp30aPrhuHHjsmPJWnHmOLmwX/J63meeeSbWxvbt2x2bC8tbuhbrZawFsu9aujbra6zj3XrrrbF9WlpahjwO3x+sJQPACy+8MGQblv+z5sh6It8PTz31VKyNTZs2AUgXi8KMpg8CbxfQyMS3WOvmOUaFfZWvi6X7ctwLj5MVo8CxD6w5++4PIO7ffFzLH/h8q6urHfsf/uEfHPvyyy+PteHLeZHveuakZHwniQ8Byt0thBBCBIsmaSGEECJQNEkLIYQQgaJJWgghhAiUVMlMRoPcxftWQJMvMMwX8ATEgxo4CCpJ4AAH8HDQgxUowUEdfFwr+T0nPLn99tsdmwNnLNIEbeVbYMMiN9DFCnoJmYGBgewYZPIy58J+xgFamWClDFYGKfYZtq2EOOx37B8caGgVB+HP/u3f/s2xP/CBD8T2+dOf/uTYHAjHbb7rXe+KtcF9P/fccx3bCja79NJLHTs3cQgA/OpXv3LsjRs3xtrI3IsjFRA0kuT6ofV88BXU4EArDhIEgDfeeMOxLZ9hOGCRA8k4MYsVtHfgwAHH5kAx63w5UPYf//EfHZsDeK1z8SVqSpJEK02ykky7SRM76Ze0EEIIESiapIUQQohA0SQthBBCBEpJCIRJ3t3zNqwVWMk9eDE7L/i39BPfgnfWU6ykAVyqjrVxK7F7c3OzY1999dWOzefvK3RhYS2u92kwrMn6Fuhb8QUhk1voxRpTji/gQhasUVt6KPvZhAkTHJv9A4j7GRewZw3urLPOirXxL//yL4593XXXOfaTTz4Z24fPp6amxrH5/FijBOIJL7hgBJ8LEC9CwgU2duzY4dgzZsyItZHx3UIkMxltjh07lo3nsPRV1pjZDzmpjlVwhZOZJNHufXE9/PyzEgJx3y688ELH/qd/+qfYPrwN40vclIQkSbSYJDE7mb5IkxZCCCFKHE3SQgghRKBokhZCCCECJVhNOrfAxmDfDwVr0FzvFYjrMlzYgDUaIL6WkNvIFJbPwOtXLVi3++IXvxjbhteJ+vSMJGv8kpDvOmlL906jB4VIT09P7DO+3i+99JJj83plSw/lMWVN2tqHtb/a2lrH5upMd955Z6wNXsPK5/LKK6/E9nniiScc+5prrnFs1oL3798fa4PXfbNv/+Y3v4ntYxXMyCXJ+vtMIY9S1KSPHz+efaa9/vrrse95/T2vPWbd11qvz2upOe7BiutheE0zP/+sQi9c/IKLuFg5IPgaplmvzCR5Tvm2SfKMzfRVmrQQQghR4miSFkIIIQJFk7QQQggRKMFq0rlY7/l97/45t6+lWfDa0Z07dzp2R0dHbB/WJVnH47XWFqxrLFmyxLEvu+yy2D6swfvOP4neUYi1g9yGddzcbUptnfSRI0ey+i9fayCuQe/atcuxWdez1p7yZ5wj/N3vfndsn3nz5jl2Y2OjY/Na+7a2tlgb7O+sSVrXknNxr1u3zrFZo16wYEGsDc73fMcddzg2jxkQvyd4jDZv3uzY1n04e/ZsAHbugtDJjY/hPAtAPDaG102zFmzFCvD1Z53feubwcdiX+Vp//OMfj7XxwQ9+0LGT5Pv2xcoUQqO2yDcmZ6jYoKQxQvolLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIlJIIHEsCJwSZNGmSY1uJSVi45wID1sJ7DrbhQKgkAQt8XA62sYKrRiLgyhcEluS4SQrDlzJdXV3ZIgJWUQJOCsGBNBwExUlHAOBTn/qUY3Nyhzlz5sT24cAfDvrhwKqXX3451gYH+XBA0g033BDbh5NV8D3CiUisoLft27c7No8rF+AAgGuvvdaxOcHF1q1bHfuSSy6JtZFJtJIkuDM0Tp48mQ2gspLqcCImvpadnZ2Ozf4CxAsO8bOMrz0QDy678cYbHfv973+/Y3PgIRB/7iQp7OELFEsTFJukDV8ho3ye/wocE0IIIUocTdJCCCFEoAT3ujvziiH3lZSVl5dfw/KrGl8eWgtuM5PrNxdes8evZpK8/uVXIvwq3noVNRrri9O8IuLz9eXuztSTDT2fd6Z/uT5gra/N1x+sV3nsZ+yrVg1e9hnfPlbf+TM+F8v/+TPf+Vqvln1tWOti+VUr3++cI9rqe6Yvmf+G7oPAO33Mvb7WmPK15PFI8pzy3cvWPvz6l/vBfmnJjr7X3UleIfteVVvXmj9L87qbt+ExGqqNzNj4/HBMFJin7tmzB/X19cXuhhhhdu/eHUs4ExLyw1Of0H0QkB+eDvj8MLhJemBgAHv37kUURWhoaMDu3btjVXNEenp7e1FfX1+0cY2iCIcPH0ZdXV2qqlyjhfxw5JAPJkd+OHKUih8G97q7rKwM55xzTvaV7+TJk+WUI0Axx5VTVoaI/HDkkQ/6kR+OPKH7Ydj/jBRCCCFOYzRJCyGEEIES7CRdUVGBr33ta7GKKGJ4aFzzQ+NVeDSm+aMxKzylMqbBBY4JIYQQ4m2C/SUthBBCnO5okhZCCCECRZO0EEIIESiapIUQQohACXaSXrlyJWbOnInx48dj4cKF2LRpU7G7VDK0trZiwYIFqKysRHV1NZYuXYqOjg5nm+PHj6O5uRnTp0/HpEmTsGzZMrM05+mO/DA98sPCIB9Mzynhg1GArFmzJiovL49+9KMfRVu3bo0++9nPRlOmTIm6u7uL3bWSYMmSJdGqVauiLVu2RM8++2x00003RQ0NDdGRI0ey29x9991RfX191NbWFm3evDm66qqroquvvrqIvQ4P+eHwkB8OH/ng8DgVfDDISbqxsTFqbm7O2v39/VFdXV3U2tpaxF6VLvv3748AROvXr4+iKIp6enqicePGRQ8//HB2m+3bt0cAog0bNhSrm8EhPyws8sP8kQ8WllL0weBed/f19aG9vR2LFy/OflZWVobFixdjw4YNRexZ6XLo0CEAwLRp0wAA7e3tOHnypDPGc+fORUNDg8b4b8gPC4/8MD/kg4WnFH0wuEn64MGD6O/vR01NjfN5TU0Nurq6itSr0mVgYAD33HMPFi1ahHnz5gEAurq6UF5ejilTpjjbaozfQX5YWOSH+SMfLCyl6oPBVcEShaW5uRlbtmzB73//+2J3RZzGyA9FsSlVHwzul/SMGTMwduzYWHRdd3c3amtri9Sr0qSlpQWPPfYYnnjiCaeoeG1tLfr6+tDT0+NsrzF+B/lh4ZAfpkM+WDhK2QeDm6TLy8sxf/58tLW1ZT8bGBhAW1sbmpqaitiz0iGKIrS0tOCRRx7B2rVrMWvWLOf7+fPnY9y4cc4Yd3R0oLOzU2P8N+SHw0d+ODzkg8PnlPDBIgeumaxZsyaqqKiIVq9eHW3bti266667oilTpkRdXV3F7lpJ8LnPfS6qqqqK1q1bF+3bty/79+abb2a3ufvuu6OGhoZo7dq10ebNm6OmpqaoqampiL0OD/nh8JAfDh/54PA4FXwwyEk6iqLou9/9btTQ0BCVl5dHjY2N0caNG4vdpZIBgPm3atWq7DbHjh2LPv/5z0dTp06NJk6cGH3kIx+J9u3bV7xOB4r8MD3yw8IgH0zPqeCDKlUphBBCBEpwmrQQQggh3kaTtBBCCBEomqSFEEKIQNEkLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIFE3SQgghRKBokhZCCCECRZO0EEIIESiapIUQQohA0SQthBBCBIomaSGEECJQNEkLIYQQgaJJWgghhAgUTdJCCCFEoGiSFkIIIQJFk7QQQggRKJqkhRBCiEDRJC2EEEIEiiZpIYQQIlA0SQshhBCBoklaCCGECBRN0kIIIUSgaJIWQgghAkWTtBBCCBEomqSFEEKIQNEkLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIFE3SQgghRKBokhZCCCECRZO0EEIIESiapIUQQohA0SQthBBCBIomaSGEECJQNEkLIYQQgaJJWgghhAgUTdJCCCFEoGiSFkIIIQJFk7QQQggRKJqkhRBCiEDRJC2EEEIEiiZpIYQQIlA0SQshhBCBoklaCCGECBRN0kIIIUSgaJIWQgghAkWTtBBCCBEomqSFEEKIQNEkLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIFE3SQgghRKBokhZCCCECZcQm6ZUrV2LmzJkYP348Fi5ciE2bNo3UoYQwkQ+KEJAfiuEwJoqiqNCNPvTQQ/jkJz+J73//+1i4cCHuv/9+PPzww+jo6EB1dfWQ+w4MDGDv3r2orKzEmDFjCt01UWSiKMLhw4dRV1eHsrKRe5EzHB8E5IenMqPlg4D8UAxOYj+MRoDGxsaoubk5a/f390d1dXVRa2urd9/du3dHAPR3iv/t3r17JFwvy3B8MIrkh6fD30j7YBTJD/U3fD88AwWmr68P7e3tWLFiRfazsrIyLF68GBs2bIhtf+LECZw4cSJrR3/7Yf+73/0OZ555ZsH6FRkvDPhfptY2vn0GBgaGtK1//fJx2O7v7897H4b7Ye3D21jHHTdunGPv27fPsf/zP//Tsa3zzf2sv78ff/nLX1BZWTlIz4dPvj4IDO6HP/3pTzFx4kQAwBlnxG8X/mzs2LGOzePH32f6NtQ+FtwOt5HmF2Ka+4FhH7LOlz/j41p+yPjOzzpuhiNHjuCqq64aUR8ECuuH//7v/47x48cDQPa/ubAf8piyT1VUVMTa4G18vm214/NLy7d5G27DOi6fH9u+8bDw9SMJ/Ey1/DTz2ZEjR3DFFVd4/bDgk/TBgwfR39+Pmpoa5/Oamhrs2LEjtn1rayu+/vWvxz4/88wzMWnSpIL161SfpH0TcJJtkkzSmQkrg++Bm89nhSJfHwQG98OJEydm/7FYiEnaaqNYk7TvQWf5kO+6vfXWW45tnW+xJ+nBjltoCumH48ePx4QJE7L/z/A4+3wqySSd5B+Y3E6+/bC2STNJcxulMEkn7VvBJ+l8WbFiBZYvX561e3t7UV9f72yT5EQZflhYA8FtWA8lH75+WBM/78MPpUI8PKx+pfn1zTfh/v37HTv3X/2AfRPmXoskD+BiMJgfjh07NnuzJvlVmGbyTDPhsI/4Jr4k/0hN0lc+zhtvvOHYPPkcP3481sbVV1/t2DzpWMf13Zv5PPhC1naT+GGSf/j7/gGW5B/OPv9Iso1v0h6sLz5815vHI809lIYk91DmuEnPu+CT9IwZMzB27Fh0d3c7n3d3d6O2tja2fUVFhfmvOiHSkq8PAvJDUXjkh6IQFDy0sby8HPPnz0dbW1v2s4GBAbS1taGpqanQhxMihnxQhID8UBSCEXndvXz5ctx+++248sor0djYiPvvvx9Hjx7Fpz/96ZE4nBAx5IMiBOSHYriMyCR966234sCBA7j33nvR1dWFyy67DI8//ngsgEKIkUI+KEJAfiiGy4gFjrW0tKClpSX1/meccUY26CBJsAEHCiSJkPVFlVrBNvkGsBQqqjzffiQJgksSVcvbdHZ2OjYH/Rw9ejTWhhXhOxoM1weBt88/n0APX0BLkgDGNBHzviCYJL7c19fn2JY2OmXKFMfmwDEOLPzDH/4Qa+P888937JkzZzq2db7sQ0mWejGZ8x3twLHR8ENfAFcaP0wSWJVv4GSS4yaJKvddwyTnmyZQLN+ljUNtn7Qt5e4WQgghAkWTtBBCCBEomqSFEEKIQCl6MpPByF28n4QkGoSPJBqBLwEK29Y5+LKSpUkakATWJU+ePOnYlnbMSWFYk+ZkJtb5HjlyJPv/aRLGFJPc2Ajr3HzJGnifJBm4klzbJEkicrH05ddee82xt2/f7j0uJyJhH2If6+3tjbWxdevWIY9r9XXOnDmOfe655zp2PomJRrqoxkjg06Tz1ZOTJKpJkvnLl9mL/cE6LmvQaeJ6fJn/0sR1WOS7z1B6etL5rfS8VQghhDhN0CQthBBCBIomaSGEECJQgtWkgXfe3ScpUpFm7XEabdinqabRwrkIgaXj8fmwVszrRi3tM1NFJwNXGSsvL4/tc+zYMcd+9dVXHZvLibJGDbjaS8jFDSzKysqG1DDTFKnIt400azz5++rq6tg2b775pmPnxg4AwM6dO2P7nHfeeY7Nvst+yWuvAeCFF15wbNaXn3nmmdg+f/zjHx37S1/6kmNPnjzZsQuxPjUkfIVefD6TT+GH3GMO9f1gn+WSJAaF4xr4OWT1NY0GnS9JihTlO+5DbTtoPxJtJYQQQohRR5O0EEIIESiapIUQQohACVaT9mmBTCHWSTNJ1udxH5Pk/2Z4H0sbZq2Xtb9Dhw459uHDh2NtsD7IGjWvRQXimjPnbt6zZ49jW3p6ri5Vauukc9enJtECfWtLrVgBn++myX3++uuvO7aVU53Xp9bV1Tn2K6+8EtuHNWi2+bhW33mtNW/D66gB4K9//atj796927EvueQSx+b7A3jnWpWiJp3rh0n05DSadL6+DMSfKZWVlY49a9Ysx967d2+sDX6mTJ061bE5hgGIx0+cddZZjs3PUNa9gXR6sm+c85mHpEkLIYQQJY4maSGEECJQNEkLIYQQgaJJWgghhAiUYAPHchfvW8FGI5EUoxBtchAY20A8qIGDy954443YPrW1tY594MABx+aAHU46AQBdXV2OzUFeGzdujO3DiSY4aQSfn3Wtcs8vSSBdqKQpbJCkjSQBOvnu8/LLLzt2e3t7rA0O2Jk3b55jW8E2HIDGwWZ8/S0/nD17tmN3dHQ4thVsxu1yYFg+SWRKMXAs93mYpliELyjM2iZJoYsZM2Y4Nl9v7sf5558fa8MHB68C8aBGDjb8u7/7O8duaGiIteFLEJVkjHzPMwWOCSGEEKcwmqSFEEKIQNEkLYQQQgRKsJq0L4kEw9pAEu2T202zDy/mT1LknNm2bZtj//rXv45twzoeJy/p7u72Hpe1HU5mwf0AgGeffdaxDx486NicZMXSE3MTXpRaMpN8CxuwzeNhXRdfcYAk15L1tIqKCsceP358rI3nnnvOsadPn+7Y1rXi6z1x4sQhj8uxFBZ8D1lFOXgMqqqqHDuf+67UirwwSZJssN8lOWdul4vrnH322bF90mjMPjjOJUlipv379zv2Qw895NisUQPAlVde6dgcX2EVC2I/G04yk6SxEfolLYQQQgSKJmkhhBAiUDRJCyGEEIESrCadSxoNKcl6tjRr3LiggE+D2LlzZ6wNXq/MWrBVDIHXMPu03XXr1sU+Y82Fz4X1Zus4rGOz1mmNaa4+Zq0bD5nhFjZIolH5/JsLoQDxNe487qwv8/p2IH4tOjs7HXvSpEmxfTheYtOmTY7Nvsxrr5P0zdICWYPnIgzMUONciuukx4wZk5fO7rOt+Ar2B9agrQI8hYCPy88h1psBYN++fY7NuSf4/DZv3hxrg3114cKFjn3ttdcO0uN34Psun1iApIVzSs9bhRBCiNMETdJCCCFEoGiSFkIIIQJFk7QQQggRKMEGjuUG7CTdPt/v+TMOkrIKDHDwBidz4OCbP//5z7E2nn/+ecd+8803HdsKvuK+8T4cXMSBFQDQ09Pj2BxsYSWR4DE666yzHHvx4sXeNh577LHs/3PSjdApKyvLBqEkSUTiS16SJiGKtQ8nr2Ff5X5YgYZ8LbhowYUXXhjbhwMhf/CDHzj2ZZdd5thW8A0f54UXXhjyewCYOXOmY3MykyQJjzKUYuBYWVlZtt9JnmU+H7La4OfOSASKWc8HLgbDgWLbt2+P7cP9Z//n8+VgRQA4cuSIY//2t791bH6WA8DHP/5xx/YFfxWi0EvpeasQQghxmqBJWgghhAiUvCfpJ598EjfffDPq6uowZswYPProo873URTh3nvvxdlnn40JEyZg8eLFsddZQgwH+aAIAfmhGA3y1qSPHj2KSy+9FHfccQduueWW2Pff+ta38J3vfAc//vGPMWvWLHz1q1/FkiVLsG3bNjPJ/2Dkq0kXAtZkLJ1rypQpjs26BmuF1jmwnnbgwAHHthJ+8NhxcfUFCxY4tqVrX3755Y7NesqLL74Y24f7zzr2BRdc4Nhc+ANwNae+vj48/fTTsW3yYbR8EMg/mUkh4CIVlp7Mvsk67rRp07zH4aQhnNzmXe96V2wfKylKLjy+9fX1sW1Yc+T4CSt5yyWXXDLkNr7CB7kUSpMulh9a5+Y7J97Hesbws20k2Lt3b+yzv/71r47NGjXH31jw+fE9xEmYgPjzr7Ky0rG3bt0a2+dnP/uZY99xxx2OzZr7UPdu0mdH3pP0jTfeiBtvvNH8Looi3H///fjKV76CD3/4wwCAn/zkJ6ipqcGjjz6K2267Ld/DCRFDPihCQH4oRoOCatK7du1CV1eXE/FbVVWFhQsXYsOGDeY+J06cQG9vr/MnRFrS+CAgPxSFRX4oCkVBJ+lMTuqamhrn85qamli+6gytra2oqqrK/lmvx4RIShofBOSHorDID0WhKPo66RUrVmD58uVZu7e3N+aYSTQkn+ZitcGaHG9jrYHjtca8bpTt119/PdYG9431NdYGgfg6wFtvvdWxr7vuOse2inRwG6zjNTQ0xPZhjYV1GtYXuRAI4BZDsIonhEASP/QVZLFIUtiAYX+w1uuzD7322muOzfqi1Qb3jbW/w4cPx/bhvAC8LvpDH/rQkNsD8fuMi3BwMQ0gHuvBfWM9sRDrU4vBYH7o06R9JDnnfHVyizfeeGNIm/VnIB4Lw28PrGcZP1cbGxuHbMPSpDm+hp+7lkbPeS+4CMkHP/hBx7buu3wpqLfW1tYCiN9Q3d3d2e+YiooKTJ482fkTIi1pfBCQH4rCIj8UhaKgk/SsWbNQW1uLtra27Ge9vb14+umn0dTUVMhDCWEiHxQhID8UhSLv191HjhxxXunu2rULzz77LKZNm4aGhgbcc889+MY3voHZs2dnlx3U1dVh6dKlhey3OI2RD4oQkB+K0SDvSXrz5s1473vfm7Uz+sntt9+O1atX48tf/jKOHj2Ku+66Cz09Pbjmmmvw+OOP5613jB07Nq/1ZKwXsgZhaaHcLq8tZT0FiGsdrFFzsgIuYG71NUnx749+9KOOfcUVVzg2azRJcvNy7mZLP+F9eI03a6G8bhxw8zcnWfPoY7R8MA087j7b+ox1ayvfMV8rvpbsd1YbrFNym9Y6aR5DjmPg9cJWdPKuXbscm33I0sJfeuklx+Z8ztdcc41jW+uAMxRqbfto+mGuJp0kroGvrS+3d6Hga7ljx44hbWufV1991bGfeeaZ2D4XX3zxkP1IEj/C48i2FRvEtQvWrVvn2MeOHXNsa4leJuYiyXMfSDFJX3/99UMOwJgxY3Dffffhvvvuy7dpIRIhHxQhID8Uo0G4YY5CCCHEaY4maSGEECJQNEkLIYQQgVL0ZCaDke/ifRb9kyTN4IXovI8VBMUBOmzzAnkrcIaD2ljX+tKXvhTb58orr3RsDsBKU9Sdt7H0Nf6Mk0bw+XMRD8ANruPAitDJN4CRA3KSBOj4kuhYSfoZDkbiwDGrHxwYxsGHVuDYjBkzHJsTvnAiFuseYp/iwCEr2JKD2qzkFD6Gkwyk2ORb6IW34eeD1cZQwXZJ4Wvb0dHh2FxMCIgnQOKEIVZfr7rqKsfmwEgusGEFafmKYVj78PN9y5Ytjv3AAw849ne+851YG3feeSeA5H6oX9JCCCFEoGiSFkIIIQJFk7QQQggRKMFq0mVlZXklyfcVj+BF6NY+nLzESu7BehkXKGdd28q9y0XuP/CBDzg2J2YA4nqhL6GBpWP6tM0kGgkXTOBkLuecc05sn9xxTaMlFpNcLdDCdx2SJDNJk1iCi1BwtSUuWmAl5uG+c7wAxxsA8SQ6fL0PHTrk2JYmzZ/5bACYNGnSkMdNMs6nE+xTSRIoWck78oVzlbPfWZo0J6rha2f1lYtu8LOa7wd+TgH+5E6sPwNAZ2enYz///POOzX767ne/O9ZGvuiXtBBCCBEomqSFEEKIQNEkLYQQQgRKSWjSlr7E+irrCUPVbM3AOgWvm7PWWvOaTtZx2LY0GF6fvWzZMse2ilCwLpOmkANrMDyGSZLSs07D66at883V5ZMmlQ8RS3/2jXuaddJJ2uBx5FgBjsFgvwXi/t/Y2OjYCxYsiO3DWh/7A2uDVrEMzh3A/m7FgtTV1Tn29OnTY9vkYl2rjH+PVHGJ0cK6T/PV5K3vk+SW8O2zZ88ex+b4AqvQy9y5cx177969jm3FubAmzT7EfsqFjwD/mFk5Lp544gnH5uc9Pw85hiMNpe2tQgghxCmMJmkhhBAiUDRJCyGEEIGiSVoIIYQIlJKI4kkSBFVdXe3YnDTDSpDAoj8nc7CCbThZA7fBAQ3Wcd/73vc69syZMx27q6srtk++AVdpApasYBtfQQwedytpwO9///vs/6cJTikmvmQmI5FEw0oi4jsuFxTgwDH2SyB+j3AQmBWgxD7D7XIyC07cA8TvKytQkuECMxwo5+sn8E6gZCkGjp1xxhnDCrrke9u61zlgj+99Lp4CxJPmcOAY+yUHzQLxZwgXcbGOywFo1157rWNz3/m5DcSDD/lZ/j//8z+xfTiZFScV4vPjwjdpKD1vFUIIIU4TNEkLIYQQgaJJWgghhAiUYDXpMWPGZLUjqxg5JzJn7YP3sTQ5/sxnA/FCF2yzJjt16tRYG3/84x8dmwuHs0YNxHU7n65mFdPgMeI2rQX/rMHwmFxwwQWOPWfOnFgbq1evzv6/lagiZKIoGjLJC3/nSxgzlFaaIUmhC9a6WE9m/2ftzNqHj2NdK+4r+zv7C98fQDxJBGuBViKiG264IfbZUAxVyKQUNenc2IgkcQ+8ja+4DhDXqS0tmNm+fbtj8/XnRCRWEQ8uwnHhhRcOeQwAeN/73ufYl112mWOznmzFMHDipZ///OeOvXv37tg+nHiFtznvvPNi+zCZZ0SS5FGAfkkLIYQQwaJJWgghhAgUTdJCCCFEoASrSff392c1MktP4M98a56t9b78Ga9pttb8subC+hrreKydW8f91re+5dj/8R//EduH9SLWD3kNZW5RiwyvvPKKYz/11FOObWkkvJaQ9SEeM9ZsAGDp0qXZ/z927Bj++7//O7bNqUKaddKsF7IPWXqitc51qO+t9ZqsSbNtrS3l9dd8P7C+aBXYYJ/h2Igbb7wxtg/r1OyrPO6WL2fGMYk+Gxo+TTrfIi2WLu/zKQvWdbmwRWdnp/cY/IzkXArTpk2L7XP77bc7NvtqkrX3XCyD2zj//PNj+/gKKp177rne4+YTWwDol7QQQggRLJqkhRBCiEDRJC2EEEIESrCadEVFRVZHs3RdXgfKGgTrXlYBb96G9TPW24C4BsH6SVVVlWNbfWedl4uc/+Y3v4nt84lPfMKx+Xw4R/LOnTtjbezYscOxWafkHLoAsHXrVsdmfWjJkiWOben4uXmXrVzmpzJJ1uT6tCkrZzNre3wczm2cJB+4rw0gHtfA60STaNJ8r3J8yfz582P7+NaU8vdWboVC5FUvFr4c8tb2ubC/WG352rdqCvD15vXKa9eudWxrnTTHOfAz5Lbbbovtw/ET/JzlmJyXX3451gY/i/j5Z9UZ4HuCx4zXeBcC/ZIWQgghAkWTtBBCCBEoeU3Sra2tWLBgASorK1FdXY2lS5eio6PD2eb48eNobm7G9OnTMWnSJCxbtixWvk6I4SA/FMVGPihGi7wm6fXr16O5uRkbN27Eb3/7W5w8eRLvf//7nXzOX/ziF/GrX/0KDz/8MNavX4+9e/filltuKXjHxemL/FAUG/mgGC3yChx7/PHHHXv16tWorq5Ge3s7rrvuOhw6dAg//OEP8eCDD2YT4q9atQoXXXQRNm7ciKuuuirxsXIDTqzEHBwYwkEAnDDEKpbBSSN4GyvYjBMhjBs3zrE5CMZKIsFtcBJ6Hmerr3z+/K94K0CL+3bRRRc59nPPPRfbhwM97rzzziH7ZSWKyN2mEAU2RtMPfQE7vqQRhcBqk8eRg1zYThLAxcVgrIQQHDzEBTQ42NJKiMJ9u/zyyx3bSiLBFDsIbDR9EMjfD31YQYEcwMWsX78+9hkHklZWVg5pW8fgwj+cICVJ8hkuBsJJVaxiGXxfJZkzOACPA4Wvv/56b1/zZVhPlMwNmLlQ7e3tOHnyJBYvXpzdZu7cuWhoaMCGDRvMNk6cOIHe3l7nT4h8kB+KYlMIHwTkhyJO6kl6YGAA99xzDxYtWoR58+YBePtf2eXl5ZgyZYqzbU1NjRm+D7yt7VRVVWX/6uvr03ZJnIbID0WxKZQPAvJDESf1JN3c3IwtW7ZgzZo1w+rAihUrcOjQoeyf9VpCiMGQH4piUygfBOSHIk6qZCYtLS147LHH8OSTTzp6am1tLfr6+tDT0+P8C7K7u9ss5A68rUewJgEA9957b1bvbWpqin1/7bXXOja/FvIlTLC2Yf3M0kIsLScXXmRv6YncbpJiCP/3f//n2KzBnHnmmY7NOjcQ11j27Nnj2H/6059i+3zpS19ybD4/1kZ951tILXE0/HDs2LHZ62MVBxgJDdpXPAXwF6ng+4GTTgDxBCDsU1ZcA+uFvuNaySv4OFxQg79Pw1DXpZDXrJA+CAzuhwMDA9n7KE2CHLatZC+sL2/bts2xX3jhhdg+//zP/+zYzz///JBtWsmd+Hz47cGLL74Y24fh8+HnoeVT/Dz0PdstODHVSLz5yMtboyhCS0sLHnnkEaxduxazZs1yvp8/fz7GjRuHtra27GcdHR3o7Ow0J1oh0iA/FMVGPihGi7x+STc3N+PBBx/EL3/5S1RWVma1laqqKkyYMAFVVVX4zGc+g+XLl2PatGmYPHkyvvCFL6CpqSnvaEYhBkN+KIqNfFCMFnlN0t/73vcAxMPMV61ahU996lMAgG9/+9soKyvDsmXLcOLECSxZsgQPPPBAQTorBCA/FMVHPihGi7wmaV+Se+BtPXXlypVYuXJl6k4BbydZz2jSP//5z2Pf575GAoA5c+Y4NkdVWrAWwms8X3311dg+vN6U10nzGCVZF5yk+AHr1KyFsF5qHZfXAT777LOObekpl156qWOzxsjHtbSuQjOafpiLdW6WXpxLknXUfK1YP/OtPbfaZX3N6iev8eTjsr9Y7XIcB2vUls538803O/bFF18c28aHT5ct5Jpii9H2wSiKsse0js2fse3L7wDE1xLzGvgPfvCDsX1Yc+Z92Kes2CC+Huedd55jcwEiAPj1r3/t2Nddd92Q/bDyBPBnHAtgxQbxuHLf2d85hicNyt0thBBCBIomaSGEECJQNEkLIYQQgaJJWgghhAiUVMlMRoOBgYFsoI61EJ0DVjZt2jRke5aAz0FPnPDBCujihPG+wKEkgQN8HCuog4/DC/E5oMsqjsCJBLgiT6YQwFD4gm6sgJ7ccbYSgpQKSfruSyKRJGiJEzFYAWsc1MLXlq+D5YccXMk+ZpVVZL/av3+/Y/M9dO6558bayM1nDdgBOkySIgunMmVlZdlravlQvn7ne24B8YRI1nE5uJBtfnZbfsh94YQ4Vv7yb37zm479i1/8wrFfeeUVx96+fXusDfa7JEGeHDjGwWZJxjVf9EtaCCGECBRN0kIIIUSgaJIWQgghAiVYTfrkyZNDJgxgjYr1A9Zod+7cGWuD9QNeaJ8kkT33kfVl1rCBeCIK7qulv504cWLIdjMl8jI0NjbG2rj88ssde8aMGY5tJa/g8+Ex4fP3jVmpadInT54cMiGNT8dK4kOsOfOYWwlBfEVZOK7B8kPWC3t6eoa0gXiBDd6G/ZT1ZwCxAhO+gjNAfBzTJCTJ7FPIIi+jxZgxY7L9tnzKd05Jzpm3SRIrw77Lz5A0SZZ27Njh2JaezMlKtmzZ4tg8RlZcUxINmvHFRoxEwR39khZCCCECRZO0EEIIESiapIUQQohACVaTzk0ob61X5s9YK2CNjosJAMCuXbscm9enWvoC69is07BGwxoeEF+fygnlJ0+eHNuH69Vee+21jn3++ec7tqXrsfZjraVmfOstkxQaKGXWrVuX9SVrjSd/xravAAsQv3bsh6y/AfF18qz1+XRuIO4PrCfzmmcgrkFzG1OnTnVs1p+BeCEbvqestabszz7tfyhtkGNATkXSrM/3tWFp0ux3bLN/WH7I/t3R0eHYr732WmwfXp9cXV3t2OxD1nH5XuR7xtLP+bg8hzz33HOOzQWK0qBf0kIIIUSgaJIWQgghAkWTtBBCCBEowWrSAwMDWZ3Z0gYsjWGo7zm3MRDXD3k9qqWNsW7HsH7y+c9/PrbNHXfc4di8XjlJfluf9mOtrWUNhnU+aw1gIXJR5x631DTsjo6O7PWwdM58c0pbebhramqG3If1ZwDo6upybM5vbK1xZlj7Zq2W8+MDfg2S2+CcyoB/TbcVT8FaIMdtcI4DSz/N5BE/cuRI7LtSpxDryJk0a6v5urAPWX7JvszPLiueyJfTgm3rechzBD9DrXuVfTOJ7w4X/ZIWQgghAkWTtBBCCBEomqSFEEKIQNEkLYQQQgRKsIFjfX19QwaO+QJ2+Pskxbg5qClJUBAHKHBgxNy5c2P7XHjhhY7Ni/mtoDgOfPAVtrCCPnznl2QfX4ENDgoCgLVr12b/3yriETLjx483A/kGI99AMiCe3IbbsILtOJiMAxY5gMu6LnwcTjJiBUmyH3IbPFZJAtj43uTEPQAwZ84cx/adXykW0UhKvsGaadtI8gzdu3evYx8+fNixObDKCujjIDAOpLWehxyg5Xu2ceChBW9jBY75nnd1dXXe4+SLfkkLIYQQgaJJWgghhAiU4F53Z14n5L5qsF47+F4r8isS6/UPf+azk26Ti/V6l9e0sp1mPW6avid5reobR7421nrE3DHI/H/o66Uz/bPOZyjSvO7mV9f8ytBa18t51315uK3x5jZ4H0tm4s/YP/j7JOPHY2bl1uYx4r6yH1qvczOvJjNthe6DwDt9zL1WSept83XgMbaura8ugTWmfF342vF1svzB51NJ8m6nebaxz7Cd5F7mbfhZPpTMmtnWK01EgXnqnj17UF9fX+xuiBFm9+7dOOecc4rdjUGRH576hO6DgPzwdMDnh8FN0gMDA9i7dy+iKEJDQwN2795tVoUS6ejt7UV9fX3RxjWKIhw+fBh1dXVDVioqNvLDkUM+mBz54chRKn4Y3OvusrIynHPOOdlXAZMnT5ZTjgDFHFcrzV9oyA9HHvmgH/nhyBO6H4b9z0ghhBDiNEaTtBBCCBEowU7SFRUV+NrXvpZoEbpIjsY1PzRehUdjmj8as8JTKmMaXOCYEEIIId4m2F/SQgghxOmOJmkhhBAiUDRJCyGEEIGiSVoIIYQIlGAn6ZUrV2LmzJkYP348Fi5ciE2bNhW7SyVDa2srFixYgMrKSlRXV2Pp0qXo6Ohwtjl+/Diam5sxffp0TJo0CcuWLUN3d3eRehwu8sP0yA8Lg3wwPaeED0YBsmbNmqi8vDz60Y9+FG3dujX67Gc/G02ZMiXq7u4udtdKgiVLlkSrVq2KtmzZEj377LPRTTfdFDU0NERHjhzJbnP33XdH9fX1UVtbW7R58+boqquuiq6++uoi9jo85IfDQ344fOSDw+NU8MEgJ+nGxsaoubk5a/f390d1dXVRa2trEXtVuuzfvz8CEK1fvz6Koijq6emJxo0bFz388MPZbbZv3x4BiDZs2FCsbgaH/LCwyA/zRz5YWErRB4N73d3X14f29nYsXrw4+1lZWRkWL16MDRs2FLFnpcuhQ4cAANOmTQMAtLe34+TJk84Yz507Fw0NDRrjvyE/LDzyw/yQDxaeUvTB4CbpgwcPor+/HzU1Nc7nNTU16OrqKlKvSpeBgQHcc889WLRoEebNmwcA6OrqQnl5OaZMmeJsqzF+B/lhYZEf5o98sLCUqg8GVwVLFJbm5mZs2bIFv//974vdFXEaIz8UxaZUfTC4X9IzZszA2LFjY9F13d3dqK2tLVKvSpOWlhY89thjeOKJJ5yi4rW1tejr60NPT4+zvcb4HeSHhUN+mA75YOEoZR8MbpIuLy/H/Pnz0dbWlv1sYGAAbW1taGpqKmLPSocoitDS0oJHHnkEa9euxaxZs5zv58+fj3Hjxjlj3NHRgc7OTo3x35AfDh/54fCQDw6fU8IHixy4ZrJmzZqooqIiWr16dbRt27borrvuiqZMmRJ1dXUVu2slwec+97moqqoqWrduXbRv377s35tvvpnd5u67744aGhqitWvXRps3b46ampqipqamIvY6POSHw0N+OHzkg8PjVPDBICfpKIqi7373u1FDQ0NUXl4eNTY2Rhs3bix2l0oGAObfqlWrstscO3Ys+vznPx9NnTo1mjhxYvSRj3wk2rdvX/E6HSjyw/TIDwuDfDA9p4IPqlSlEEIIESjBadJCCCGEeBtN0kIIIUSgaJIWQgghAkWTtBBCCBEomqSFEEKIQNEkLYQQQgSKJmkhhBAiUDRJCyGEEIGiSVoIIYQIFE3SQgghRKBokhZCCCECRZO0EEIIESj/D9eME8WoD3mpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(2,3) \n",
    "f.set_size_inches(5, 5)\n",
    "k = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i,j].imshow(X_train[k] , cmap = \"gray\")\n",
    "        k += 1\n",
    "    plt.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46862fd-112d-43b3-bede-4266a3cd8660",
   "metadata": {},
   "source": [
    "### Convert Integer Labels (0-24) to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2656e87-6ad4-475d-8e8c-799823171b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_label)\n",
    "y_test = lb.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248366ef-60ef-4d2d-b7a3-7b2ea7b054e5",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0d53d96-4f69-4bcd-85ee-941c91d273d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6246706203880206"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dad906b8-a1f8-4e6b-be5d-dc5734c9cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=10,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2dd6932c-1ee6-437f-8767-57b2796c3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 0,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  shear_range=0,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ef8ddce-aa12-40fd-9a2b-454130701cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17090a2f-1aa7-4399-967c-fbca988c8dde",
   "metadata": {},
   "source": [
    "# Creating our Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1422df3f-93b3-42fd-8ede-514521b841e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 128)       3328      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 32)          8224      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 360312 (1.37 MB)\n",
      "Trainable params: 360312 (1.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv Layer 1\n",
    "model.add(Conv2D(128, kernel_size=(5,5), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "\n",
    "# Conv Layer 2\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(32, kernel_size=(2,2), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512,activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=24,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3c133ad-fd13-41da-af9c-c2d48faaf8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 28, 28, 75)        300       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 14, 14, 75)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 14, 14, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 7, 7, 50)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 7, 7, 25)          100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 4, 4, 25)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264049 (1.01 MB)\n",
      "Trainable params: 263749 (1.01 MB)\n",
      "Non-trainable params: 300 (1.17 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d3a32eb-a860-42ed-885d-5a32db2c09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21b9fb-8e62-465e-8d70-d9fdd68f9f34",
   "metadata": {},
   "source": [
    "### Fit the Model w/ Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831d72d-003b-4cf6-a7e5-e399f958c4d2",
   "metadata": {},
   "source": [
    "## Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab6c1430-4c7e-40d8-bd82-3f8943612245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 28, 28, 128)       3328      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 14, 14, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 14, 14, 64)        32832     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 7, 7, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 7, 7, 32)          8224      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 4, 4, 32)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 319352 (1.22 MB)\n",
      "Trainable params: 319352 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128,kernel_size=(5,5),\n",
    "                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\n",
    "model.add(Conv2D(64,kernel_size=(2,2),\n",
    "                strides=1,activation='relu',padding='same'))\n",
    "model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "model.add(Conv2D(32,kernel_size=(2,2),\n",
    "                strides=1,activation='relu',padding='same'))\n",
    "model.add(MaxPool2D((2,2),2,padding='same'))\n",
    "          \n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512,activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=24,activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d99552d-1c14-40cd-95d3-71c501343a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "138/138 [==============================] - 31s 221ms/step - loss: 2.2877 - accuracy: 0.2971 - val_loss: 1.0162 - val_accuracy: 0.6744 - lr: 5.0000e-04\n",
      "Epoch 2/35\n",
      "138/138 [==============================] - 31s 222ms/step - loss: 0.9580 - accuracy: 0.6801 - val_loss: 0.5077 - val_accuracy: 0.8410 - lr: 5.0000e-04\n",
      "Epoch 3/35\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.5631 - accuracy: 0.8099 - val_loss: 0.3724 - val_accuracy: 0.8635 - lr: 5.0000e-04\n",
      "Epoch 4/35\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.4037 - accuracy: 0.8655 - val_loss: 0.2781 - val_accuracy: 0.9098 - lr: 5.0000e-04\n",
      "Epoch 5/35\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.3127 - accuracy: 0.8987 - val_loss: 0.2394 - val_accuracy: 0.9197 - lr: 5.0000e-04\n",
      "Epoch 6/35\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.2510 - accuracy: 0.9177 - val_loss: 0.1364 - val_accuracy: 0.9631 - lr: 5.0000e-04\n",
      "Epoch 7/35\n",
      "138/138 [==============================] - 30s 218ms/step - loss: 0.1985 - accuracy: 0.9374 - val_loss: 0.1100 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 8/35\n",
      "138/138 [==============================] - 30s 218ms/step - loss: 0.1703 - accuracy: 0.9450 - val_loss: 0.0940 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 9/35\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.1452 - accuracy: 0.9531 - val_loss: 0.1122 - val_accuracy: 0.9709 - lr: 5.0000e-04\n",
      "Epoch 10/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9599\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.1237 - accuracy: 0.9599 - val_loss: 0.1052 - val_accuracy: 0.9713 - lr: 5.0000e-04\n",
      "Epoch 11/35\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.1002 - accuracy: 0.9705 - val_loss: 0.0646 - val_accuracy: 0.9888 - lr: 2.5000e-04\n",
      "Epoch 12/35\n",
      "138/138 [==============================] - 31s 228ms/step - loss: 0.0917 - accuracy: 0.9721 - val_loss: 0.0700 - val_accuracy: 0.9835 - lr: 2.5000e-04\n",
      "Epoch 13/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9749\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "138/138 [==============================] - 30s 218ms/step - loss: 0.0830 - accuracy: 0.9749 - val_loss: 0.0527 - val_accuracy: 0.9879 - lr: 2.5000e-04\n",
      "Epoch 14/35\n",
      "138/138 [==============================] - 30s 218ms/step - loss: 0.0735 - accuracy: 0.9779 - val_loss: 0.0760 - val_accuracy: 0.9813 - lr: 1.2500e-04\n",
      "Epoch 15/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9789\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0702 - accuracy: 0.9789 - val_loss: 0.0587 - val_accuracy: 0.9881 - lr: 1.2500e-04\n",
      "Epoch 16/35\n",
      "138/138 [==============================] - 30s 219ms/step - loss: 0.0691 - accuracy: 0.9793 - val_loss: 0.0620 - val_accuracy: 0.9841 - lr: 6.2500e-05\n",
      "Epoch 17/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9801\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "138/138 [==============================] - 31s 222ms/step - loss: 0.0667 - accuracy: 0.9801 - val_loss: 0.0530 - val_accuracy: 0.9879 - lr: 6.2500e-05\n",
      "Epoch 18/35\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0615 - accuracy: 0.9827 - val_loss: 0.0497 - val_accuracy: 0.9884 - lr: 3.1250e-05\n",
      "Epoch 19/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9826\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0597 - accuracy: 0.9826 - val_loss: 0.0535 - val_accuracy: 0.9880 - lr: 3.1250e-05\n",
      "Epoch 20/35\n",
      "138/138 [==============================] - 30s 221ms/step - loss: 0.0619 - accuracy: 0.9821 - val_loss: 0.0584 - val_accuracy: 0.9865 - lr: 1.5625e-05\n",
      "Epoch 21/35\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9827\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0603 - accuracy: 0.9827 - val_loss: 0.0556 - val_accuracy: 0.9872 - lr: 1.5625e-05\n",
      "Epoch 22/35\n",
      "138/138 [==============================] - 31s 221ms/step - loss: 0.0588 - accuracy: 0.9828 - val_loss: 0.0536 - val_accuracy: 0.9872 - lr: 1.0000e-05\n",
      "Epoch 23/35\n",
      "138/138 [==============================] - 31s 222ms/step - loss: 0.0618 - accuracy: 0.9822 - val_loss: 0.0543 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 24/35\n",
      "138/138 [==============================] - 31s 222ms/step - loss: 0.0585 - accuracy: 0.9835 - val_loss: 0.0538 - val_accuracy: 0.9876 - lr: 1.0000e-05\n",
      "Epoch 25/35\n",
      "138/138 [==============================] - 31s 227ms/step - loss: 0.0589 - accuracy: 0.9827 - val_loss: 0.0532 - val_accuracy: 0.9879 - lr: 1.0000e-05\n",
      "Epoch 26/35\n",
      "138/138 [==============================] - 31s 222ms/step - loss: 0.0596 - accuracy: 0.9825 - val_loss: 0.0572 - val_accuracy: 0.9875 - lr: 1.0000e-05\n",
      "Epoch 27/35\n",
      "138/138 [==============================] - 33s 235ms/step - loss: 0.0577 - accuracy: 0.9832 - val_loss: 0.0517 - val_accuracy: 0.9893 - lr: 1.0000e-05\n",
      "Epoch 28/35\n",
      "138/138 [==============================] - 33s 236ms/step - loss: 0.0589 - accuracy: 0.9829 - val_loss: 0.0566 - val_accuracy: 0.9870 - lr: 1.0000e-05\n",
      "Epoch 29/35\n",
      "138/138 [==============================] - 32s 231ms/step - loss: 0.0576 - accuracy: 0.9832 - val_loss: 0.0556 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 30/35\n",
      "138/138 [==============================] - 31s 226ms/step - loss: 0.0583 - accuracy: 0.9830 - val_loss: 0.0530 - val_accuracy: 0.9884 - lr: 1.0000e-05\n",
      "Epoch 31/35\n",
      "138/138 [==============================] - 32s 230ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9888 - lr: 1.0000e-05\n",
      "Epoch 32/35\n",
      "138/138 [==============================] - 31s 223ms/step - loss: 0.0593 - accuracy: 0.9821 - val_loss: 0.0521 - val_accuracy: 0.9881 - lr: 1.0000e-05\n",
      "Epoch 33/35\n",
      "138/138 [==============================] - 31s 224ms/step - loss: 0.0563 - accuracy: 0.9841 - val_loss: 0.0548 - val_accuracy: 0.9866 - lr: 1.0000e-05\n",
      "Epoch 34/35\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0545 - accuracy: 0.9838 - val_loss: 0.0538 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 35/35\n",
      "138/138 [==============================] - 31s 221ms/step - loss: 0.0559 - accuracy: 0.9838 - val_loss: 0.0566 - val_accuracy: 0.9872 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dfe8988880>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(X_train,y_train,batch_size=200),\n",
    "         epochs = 35,\n",
    "          validation_data=(X_test,y_test),\n",
    "          shuffle=1,\n",
    "          callbacks = [learning_rate_reduction]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d875199-9071-477d-9565-b21223bc48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 3s 12ms/step - loss: 0.0566 - accuracy: 0.9872\n",
      "Accuracy of the model is -  98.71723651885986 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6c50f77-47ce-4814-a0b2-36e2b12e7e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\sc4031\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"asl_mnist_98.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ae163-c243-4703-90ba-29d95f298d1d",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d9d14bb-fe38-46ae-90a9-67e90971acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 41s 183ms/step - loss: 1.0797 - accuracy: 0.6650 - val_loss: 3.2743 - val_accuracy: 0.1482 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.2018 - accuracy: 0.9330 - val_loss: 1.1774 - val_accuracy: 0.6065 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.0964 - accuracy: 0.9691 - val_loss: 0.1687 - val_accuracy: 0.9431 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 40s 184ms/step - loss: 0.0608 - accuracy: 0.9807 - val_loss: 0.0292 - val_accuracy: 0.9921 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 0.0434 - accuracy: 0.9868 - val_loss: 0.0363 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9887\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.0519 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 39s 183ms/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.0059 - val_accuracy: 0.9990 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0185 - val_accuracy: 0.9943 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.0080 - val_accuracy: 0.9978 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 38s 175ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0084 - val_accuracy: 0.9961 - lr: 2.5000e-04\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0056 - val_accuracy: 0.9989 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 38s 175ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0022 - val_accuracy: 0.9999 - lr: 1.2500e-04\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9982\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 38s 179ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 38s 174ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 8.8574e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "215/215 [==============================] - 38s 174ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 8.5548e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "215/215 [==============================] - 39s 180ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 8.2936e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 7.5860e-04 - val_accuracy: 1.0000 - lr: 1.5625e-05\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9987\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "215/215 [==============================] - 38s 176ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 7.9849e-04 - val_accuracy: 1.0000 - lr: 1.5625e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a2d72ac880>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=128),\n",
    "          epochs = 20,\n",
    "          validation_data=(X_test, y_test),\n",
    "          shuffle=1,\n",
    "          callbacks = [learning_rate_reduction]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb5f04bb-7f61-4cd8-a1db-cd4b03517b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 3s 14ms/step - loss: 7.9849e-04 - accuracy: 1.0000\n",
      "Accuracy of the model is -  100.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a830d41-1523-4294-acee-ce5ef039d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\sc4031\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"asl_mnist_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa5b29-f2ff-4933-911d-b68244352b9c",
   "metadata": {},
   "source": [
    "## Attempt 1 (Fail?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "902496f0-d4e6-4ee5-ab92-08590b8ee493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 45s 199ms/step - loss: 3.1769 - accuracy: 0.0471 - val_loss: 3.2569 - val_accuracy: 0.0201\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 42s 194ms/step - loss: 3.1760 - accuracy: 0.0463 - val_loss: 3.2713 - val_accuracy: 0.0201\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 39s 180ms/step - loss: 3.1760 - accuracy: 0.0460 - val_loss: 3.2698 - val_accuracy: 0.0201\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 39s 180ms/step - loss: 3.1759 - accuracy: 0.0453 - val_loss: 3.2767 - val_accuracy: 0.0201\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 3.1761 - accuracy: 0.0467 - val_loss: 3.2727 - val_accuracy: 0.0201\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 39s 179ms/step - loss: 3.1760 - accuracy: 0.0453 - val_loss: 3.2780 - val_accuracy: 0.0201\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 39s 179ms/step - loss: 3.1759 - accuracy: 0.0460 - val_loss: 3.2783 - val_accuracy: 0.0201\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 38s 178ms/step - loss: 3.1759 - accuracy: 0.0457 - val_loss: 3.2723 - val_accuracy: 0.0201\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 40s 185ms/step - loss: 3.1759 - accuracy: 0.0459 - val_loss: 3.2791 - val_accuracy: 0.0201\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 39s 180ms/step - loss: 3.1758 - accuracy: 0.0469 - val_loss: 3.2773 - val_accuracy: 0.0201\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 39s 181ms/step - loss: 3.1758 - accuracy: 0.0460 - val_loss: 3.2822 - val_accuracy: 0.0201\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 39s 181ms/step - loss: 3.1758 - accuracy: 0.0465 - val_loss: 3.2803 - val_accuracy: 0.0201\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 3.1758 - accuracy: 0.0468 - val_loss: 3.2807 - val_accuracy: 0.0201\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 3.1758 - accuracy: 0.0471 - val_loss: 3.2802 - val_accuracy: 0.0201\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 39s 181ms/step - loss: 3.1758 - accuracy: 0.0466 - val_loss: 3.2804 - val_accuracy: 0.0201\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 41s 189ms/step - loss: 3.1758 - accuracy: 0.0471 - val_loss: 3.2810 - val_accuracy: 0.0201\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 40s 186ms/step - loss: 3.1758 - accuracy: 0.0472 - val_loss: 3.2808 - val_accuracy: 0.0201\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 39s 180ms/step - loss: 3.1757 - accuracy: 0.0471 - val_loss: 3.2828 - val_accuracy: 0.0201\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 39s 182ms/step - loss: 3.1757 - accuracy: 0.0472 - val_loss: 3.2845 - val_accuracy: 0.0201\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 40s 188ms/step - loss: 3.1757 - accuracy: 0.0473 - val_loss: 3.2850 - val_accuracy: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a2c3682be0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(X_train, y_train, batch_size=128),\n",
    "          epochs = 20,\n",
    "          validation_data=(X_test, y_test),\n",
    "          shuffle=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991bafb-0e38-4736-80af-aff408670012",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18c65d53-dea0-42ab-b156-52cc9a12bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 15.658114850521088%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {score[1]*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b74da-aa0c-4474-99cd-997926708fa4",
   "metadata": {},
   "source": [
    "# Webcam Test with Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e09ccc-e26c-4c25-a4c7-88f4a3e43a11",
   "metadata": {},
   "source": [
    "## Load Model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f22b723c-04b0-404e-a906-523b4ac46c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./output/asl_mnist_100.h5\")\n",
    "# model = load_model(\"./output/asl_mnist_98.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893bee8-5598-4637-aa10-6944c073fa41",
   "metadata": {},
   "source": [
    "## Setup Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37c19292-7725-4b72-83e9-e7cbf8e4b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ad841-cdcd-4712-91de-1fe301305cd6",
   "metadata": {},
   "source": [
    "## Pre-Process Image/Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d322e7ad-af20-47bc-b7f4-7bc544c48e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(frame):\n",
    "    # Convert image to RGB format\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Resize (just need 28x28 since our dataset has a total of 784 pixels)\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "\n",
    "    # Reshape\n",
    "    # img = np.reshape(img, (-1, 28, 28, 1))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d3fd895-0d66-498c-b9b7-e66850b772e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_to_pixels(img):\n",
    "    pixel_list = []\n",
    "    rows, cols = img.shape\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            pixel_value = img[i, j]\n",
    "            pixel_list.append(pixel_value)\n",
    "\n",
    "    img_pd = pd.DataFrame(pixel_list).T\n",
    "\n",
    "    # Generate column names (784 columns for each pixel)\n",
    "    pixel_cols = []\n",
    "    for val in range(784):\n",
    "        pixel_cols.append(val)\n",
    "\n",
    "    # Set pd columns as pixel numbers\n",
    "    img_pd.columns = pixel_cols\n",
    "\n",
    "    predict_data = img_pd.values\n",
    "    predict_data = predict_data.reshape(-1, 28, 28, 1)\n",
    "    predict_data = predict_data / 255 # Normalize to [0, 1]\n",
    "\n",
    "    return predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec419f3-2d58-4208-bf12-f25db92044bd",
   "metadata": {},
   "source": [
    "## Load Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7c1478aa-75d4-4408-b8ee-af1e9b79e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "label_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff50c8-c56f-4f80-b485-ebea4014b45c",
   "metadata": {},
   "source": [
    "## Use cvzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fe94cb60-79ce-490c-87dd-6e57f5ebe015",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "IMG_SIZE = 300\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    hands, frame = detector.findHands(frame)\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        # Get bounding box for detected hand\n",
    "        offset = 20\n",
    "        frame_crop = frame[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "        frame_crop_shape = frame_crop.shape\n",
    "        \n",
    "        frame_white = np.ones((IMG_SIZE, IMG_SIZE, 3), np.uint8) * 255\n",
    "    \n",
    "        aspect_ratio = h/w\n",
    "\n",
    "        if aspect_ratio > 1:\n",
    "            k = IMG_SIZE/h\n",
    "            w_cal = math.ceil(k*w)\n",
    "            w_gap = math.ceil((IMG_SIZE-w_cal)/2)\n",
    "\n",
    "            frame_resize = cv2.resize(frame_crop, (w_cal, IMG_SIZE))\n",
    "            frame_resize_shape = frame_resize.shape\n",
    "            \n",
    "            if frame_crop_shape[0] <= 300 and frame_crop_shape[1] <= 300:\n",
    "                frame_white[:, w_gap:w_cal+w_gap] = frame_resize\n",
    "\n",
    "        else:\n",
    "            k = IMG_SIZE/w\n",
    "            h_cal = math.ceil(k*h)\n",
    "            h_gap = math.ceil((IMG_SIZE-h_cal)/2)\n",
    "\n",
    "            frame_resize = cv2.resize(frame_crop, (IMG_SIZE, h_cal))\n",
    "            frame_resize_shape = frame_resize.shape\n",
    "            \n",
    "            if frame_crop_shape[0] <= 300 and frame_crop_shape[1] <= 300:\n",
    "                frame_white[h_gap:h_cal+h_gap, :] = frame_resize\n",
    "\n",
    "        cv2.imshow(\"Cropped\", frame_crop)\n",
    "        cv2.imshow(\"White\", frame_white)\n",
    "        \n",
    "    \n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break \n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf583bd6-6e55-4853-a7e9-5be5b86accb5",
   "metadata": {},
   "source": [
    "## Use Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d70c27-3be5-4c60-8d93-9fb39deb7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "label_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "\n",
    "\n",
    "with mp_hands.Hands() as hands:\n",
    "    while True:\n",
    "        # Capture frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        h, w, c = frame.shape\n",
    "\n",
    "        # img = preprocess_image(frame)\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('f'):\n",
    "        # Check Left or Right\n",
    "        #print(f\"Handedness: ${results.multi_handedness}\")\n",
    "\n",
    "        hand_landmarks = results.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            analysis_frame = frame\n",
    "\n",
    "            for hand_lm in hand_landmarks:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lm in hand_lm.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                        \n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                mp_drawing.draw_landmarks(frame, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Webcam', frame)\n",
    "    \n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break \n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1b75230-5586-41b7-bd93-14ff11fa21ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x_min, y_min), (x_max, y_max), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m     mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, hand_lm, mp_hands\u001b[38;5;241m.\u001b[39mHAND_CONNECTIONS)\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImageCrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysis_frame\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_min\u001b[49m\u001b[43m:\u001b[49m\u001b[43my_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_min\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx_max\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    for hand_lm in hand_landmarks:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        x_max = 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m        mp_drawing.draw_landmarks(frame, hand_lm, mp_hands.HAND_CONNECTIONS)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "label_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "\n",
    "\n",
    "with mp_hands.Hands() as hands:\n",
    "    while True:\n",
    "        # Capture frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        h, w, c = frame.shape\n",
    "\n",
    "        # img = preprocess_image(frame)\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('f'):\n",
    "        # Check Left or Right\n",
    "        #print(f\"Handedness: ${results.multi_handedness}\")\n",
    "\n",
    "        hand_landmarks = results.multi_hand_landmarks\n",
    "        if hand_landmarks:\n",
    "            analysis_frame = frame\n",
    "\n",
    "            for hand_lm in hand_landmarks:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lm in hand_lm.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                        \n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                mp_drawing.draw_landmarks(frame, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('f'):\n",
    "                '''\n",
    "                for hand_lm in hand_landmarks:\n",
    "                    x_max = 0\n",
    "                    y_max = 0\n",
    "                    x_min = w\n",
    "                    y_min = h\n",
    "                    for lm in hand_lm.landmark:\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        if x > x_max:\n",
    "                            x_max = x\n",
    "                        if x < x_min:\n",
    "                            x_min = x\n",
    "                        if y > y_max:\n",
    "                            y_max = y\n",
    "                        if y < y_min:\n",
    "                            y_min = y\n",
    "                            \n",
    "                    y_min -= 20\n",
    "                    y_max += 20\n",
    "                    x_min -= 20\n",
    "                    x_max += 20\n",
    "                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                    mp_drawing.draw_landmarks(frame, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
    "                    '''\n",
    "    \n",
    "                # Set Frame to Analyse as Cropped Hand\n",
    "                # analysis_frame = cv2.cvtColor(frame[y_min:y_max, x_min:x_max], cv2.COLOR_BGR2GRAY)'\n",
    "\n",
    "                \n",
    "                analysis_frame = cv2.cvtColor(analysis_frame, cv2.COLOR_BGR2GRAY)\n",
    "                analysis_frame = analysis_frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                if(analysis_frame.size):\n",
    "                    analysis_frame = cv2.resize(analysis_frame, (28, 28))\n",
    "    \n",
    "                    # Pre-process image for prediction \n",
    "                    # analysis_frame = preprocess_image(analysis_frame)\n",
    "    \n",
    "                    predict_frame = convert_img_to_pixels(analysis_frame)\n",
    "            \n",
    "                    # Prediction\n",
    "                    predictions = model.predict(predict_frame)\n",
    "                    pred_array = np.array(predictions[0])\n",
    "    \n",
    "                    label_list_dict = {label_list[i]: pred_array[i] for i in range(len(label_list))}\n",
    "    \n",
    "                    pred_array_ordered = sorted(pred_array, reverse=True)\n",
    "\n",
    "                    print(pred_array_ordered[0])\n",
    "    \n",
    "                    for key, value in label_list_dict.items():\n",
    "                        print(value)\n",
    "                        if value == pred_array_ordered[0]:\n",
    "                            #print(\"Predicted Character 1: \", key)\n",
    "                            #print('Confidence 1: ', 100*value)\n",
    "\n",
    "                            if(key == \"P\"):\n",
    "                                cv2.putText(frame, key, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            else:\n",
    "                                cv2.putText(frame, key, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            \n",
    "                        #elif value == pred_array_ordered[1]:\n",
    "                            #print(\"Predicted Character 2: \", key)\n",
    "                            #print('Confidence 2: ', 100*value)\n",
    "                            \n",
    "                        #elif value == pred_array_ordered[2]:\n",
    "                            #print(\"Predicted Character 3: \", key)\n",
    "                            #print('Confidence 3: ', 100*value)\n",
    "\n",
    "                    \n",
    "    \n",
    "                    '''\n",
    "                    predicted_class_index = np.argmax(predictions)\n",
    "                    # Map the predicted index to class label\n",
    "                    predicted_label = labels[predicted_class_index]\n",
    "                    \n",
    "                    # Print the predicted label\n",
    "                    print(\"Predicted label:\", predicted_label)\n",
    "                    '''\n",
    "                    \n",
    "                    '''\n",
    "                    predarray = np.array(prediction[0])\n",
    "                    letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
    "                    predarrayordered = sorted(predarray, reverse=True)\n",
    "                    high1 = predarrayordered[0]\n",
    "                    high2 = predarrayordered[1]\n",
    "                    high3 = predarrayordered[2]\n",
    "                    for key,value in letter_prediction_dict.items():\n",
    "                        if value==high1:\n",
    "                            print(\"Predicted Character 1: \", key)\n",
    "                            print('Confidence 1: ', value)\n",
    "    \n",
    "                    '''\n",
    "                    # time.sleep(3)\n",
    "        \n",
    "        cv2.imshow('Webcam', frame)\n",
    "    \n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break \n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc577e-6fc9-43bb-9888-52d0bf7a0211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
